{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to ARINA:  The Scientific Computing Facility of the IZO-SGIker (UPV/EHU)","text":""},{"location":"#aim","title":"AIM","text":"<p>The Scientific Computing Service provides high performance computing resources, specialized technical support and consulting to researchers at the UPV/EHU, other public organizations and companies. The service provides its own computing resources with powerful hardware, upgrade policies and distributes the computational resources. The service analyzes new tendencies in HPC (High Performance Computing) for its possible implementation in the UPV / EHU. The IZO-SGI is part of the General Research Services (SGIker) of the UPV / EHU, and therefore complies with the protocols established by the SGIker. All results published using the IZO-SGI resources should acknowledgment its usage and send a copy of the publication. This publications will be used to evaluate the users when prioritizing the use of the Service. The phrase we propose for the acknowledgments is:</p> <p>Link to the acknowledge phrase</p> <p>Like for the rest of the SGIker Services, the Governing Council of the University will annually approve the table of rates to be applied to the services with specifications of the different modalities, according to the nature and institutional relationship of the users.</p> <p>The rates, at a minimum, must cover the costs of consumables used in the provision and basic maintenance of the equipment and facilities.</p> <p>You can consult the Public Price Catalog at the last page of this link.</p>"},{"location":"prices/","title":"Publice Price Catalog","text":"<p>The SGIker Services and the Governing Council of the University will annually approve the table of rates to be applied to the services with specifications of the different modalities, according to the nature and institutional relationship of the users.</p> <p>The rates, at a minimum, must cover the costs of consumables used in the provision and basic maintenance of the equipment and facilities.</p> <p>You can consult the Public Price Catalog at the last page of this link.</p>"},{"location":"prices/#cpugpu-cost-per-hour","title":"CPU/GPU Cost per hour","text":"UPV/EHU ResearchersOPIs : Organismos P\u00fablicos de Investigaci\u00f3nExternal Companies Label Concept Unit Cost (\u20ac / cpuh) INFO01001 CPU usage CPU / hour (cpuh) 0.001 INFO01002 GPU usage GPU / hour (cpuh) 0.032 Label Concept Unit Cost (\u20ac / cpuh) INFO01001 CPU usage CPU / hour (cpuh) 0.006 INFO01002 GPU usage GPU / hour (cpuh) 0.236 Label Concept Unit Cost (\u20ac / cpuh) INFO01001 CPU usage CPU / hour (cpuh) 0.010 INFO01002 GPU usage GPU / hour (cpuh) 0.356 <p>GPU/h cost </p> <p>Rates for GPU in Public Price Catalog are stablished for fraction of GPU. In that case, each GPU was fragmented in 4 parts.</p>"},{"location":"prices/#disk-storage","title":"Disk storage","text":"<p>The ARINA facility is intended for high computational demaining tasks. For this reason, and in order to avoid issues between users, the Scientific Computing service established a cost of data storage depending on the CPU usage. </p> <p>The prices are defined for data exceeding a mean value of more than 3GB / month.  The units are gigabytes (GB) .  </p> Label Anual CPU usage  (days) Unit Cost (\u20ac / GB) INFO02001 2500  &lt; cpu GB 0.11 INFO02002 100  &lt; cpu &lt; 2500 GB 0.54 INFO02003 1  &lt; cpu &lt; 100 GB 1.08 INFO02004 0  &lt; cpu &lt; 1 GB 1.62"},{"location":"prices/#taxes","title":"Taxes","text":"<p> Taxes </p> <p>Prices shown in the above tables are established without taxes (IVA). For OPIs and external centers a 21% of taxes must be added.</p>"},{"location":"access/account/","title":"ARINA accounts","text":"<p>To access the high-performance computing (HPC) resources at Scientific Computing Service of the SGIker (UPV/EHU), you need a valid ARINA cluster account. If you don't have one yet, please apply for an account.</p> <p>Although we can provide access to the ARINA cluster for all members of the scientific community, one have to tak e different procedures depending if the researcher belong or not to the University of the Basque Country:</p>"},{"location":"access/account/#upvehu-researchers","title":"UPV/EHU Researchers","text":"<p>The request of an account have to be done by completing the required form available at the following link.</p> <p>Login with LDAP account of the UPV/EHU</p> <p>In order to access to the form, you need a valid EHU/UPV credentials. If you done have, please, contact the support and administration teams at izo-sgi@ehu.eus .</p> Time of calculation requestExperienced or Permanent researchersPhD, Master or other students <p>If your group still don't have an account in our service, we strongly recommend to fulfill the form \"Time of calculation request\" with the data of the new principal investigator and the information concerning the research line of the group. Besides, in order to consider your resource demands for future extensions of the service, we request you for an estimation of the CPU/GPU time. </p> <p>The data that will be required is: </p> <pre><code>- Head of the group (ID/DNI number (without letters)):\n- Computing time request (days) :\n- Title:\n- Summary of the research line:\n- References:\n</code></pre> <p>Experienced researchers, that have been authorized by the IP of the group, can directly apply for an account by selecting the \"New account request\" button from the left banner.</p> <p>The required data will be automatically loaded from the corporative LDAP account, and then you just have to select the head of the research group from the list.</p> <p>For PhD, Master or other kind of student, the form must be fulfilled by the IP of the group to guarantee the agreement of the usage condiction of the ARINA cluster. In that case, please choose the  \"New account request with guarantee\" button located on the left banner. </p> <p>The data that will be required is: </p> <pre><code>- ID/DNI/NIE number (without letters):\n- Name:                 First Surname:      Second Surname:\n- Telephone:            email: \n- Department:           Title: (Researcher/Student/Guest/Others)\n- Head of research group: (select one in the list)\n</code></pre>"},{"location":"access/account/#opis-external-researchers","title":"OPIs &amp; External Researchers","text":"<p>Researchers of external companies have to contact the administrators to create an account to get access to the EHU/UPV network. </p> <p>Please, send an email with the following data to izo-sgi@ehu.eus : </p> <pre><code>- DNI/NIE: \n- Name:         First Surname:          Second Surname:\n- email: \n- Company and department:\n</code></pre> <p>Once you receive the credentials as invited researcher, you will be required to modify your password by accessing to BILATU. </p> <p>The next step will be to follow the same procedure described for the UPV/EHU members.</p> <p> </p> <p>Please note that account approvals are not automatic and are subject to an internal review process.</p>"},{"location":"access/connect/","title":"Connection","text":""},{"location":"access/connect/#ssh-protocol","title":"SSH Protocol","text":"<p>Secure Shell (SSH) is a cryptographic network protocol that facilitates secure remote access to computer systems or servers over insecure networks. It establishes a secure channel for data communication between two devices, protecting transmitted data from unauthorized access, tampering, and eavesdropping.</p> <p>SSH is extensively used for remote administration, secure file transfers, and other network services. Key benefits of SSH include:</p> <ul> <li>Encrypted Communication : SSH encrypts all data exchanged between the client and server, ensuring confidentiality and integrity.</li> <li>Strong Authentication: SSH supports various authentication methods, such as passwords, public key authentication, and Kerberos-based authentication.</li> <li>Flexibility: SSH can be integrated with different network protocols and applications, making it highly adaptable to various use cases.</li> </ul>"},{"location":"access/connect/#ssh-in-the-context-of-hpc","title":"SSH in the Context of HPC","text":"<p>In High Performance Computing (HPC) environments, researchers and engineers frequently need to access remote clusters and supercomputers for complex simulations, data processing, and analysis. Using SSH in HPC systems offers several advantages:</p> <ul> <li>Security: HPC systems often handle sensitive research data, making security a top priority. SSH ensures that data and system access remain secure.</li> <li>Scalability: SSH enables users to manage and interact with multiple nodes in an HPC cluster, simplifying the deployment and control of large-scale computing resources.</li> <li>Remote Access: Users can access HPC systems from anywhere with an internet connection, enhancing collaboration and productivity.</li> </ul>"},{"location":"access/connect/#login-servers-connection","title":"Login servers connection","text":"<p> Internal Network </p> <p>Notice that before trying to connect via '''ssh''' to any of the ARINA's login servers, you must be connected into the EHU/UPV network. So, either first you connect from a PC located in the EHU/UPV or you must first connect to the VPN. For the latter, you can find information in the following link.</p> Linux / MacWindows <p>If the workstation employed for logging in Arina deploys either a Unix/linux or a Mac OS operating system, then the sole ssh protocol can be used.</p> <p>Within the most common releases of these OSs, the ssh protocol is usually already activated.  If not, look for instructions on how to activate it on the web.</p> <p>You an connect easyl following the next steps:</p> <ol> <li> <p>Open a command line windows (by either clicking on the command line icon or typing command line in the OS browser bar)</p> </li> <li> <p>Use ssh command to start secure connectio to the server:</p> <pre><code>ssh -X username@agamede.lgp.ehu.es\n</code></pre> </li> </ol> <p>The -X flag allows for the opening of Graphical User Interfaces (GUIs), namely program windows. In some cases, if you are working on a Mac OS workstation, you may need a third-party application to be able to launch GUIs: XQuartz. After its download and installation, make sure to launch it before trying to log in Arina.</p> <p>In order to connect from a Windows PC, you will need to download and install a third party software that enable ssh and or ftp connection to a Linux server.  For its simplicity we recommend to install MobaXterm</p> <p>Here we show how to set up MobaXterm, which is the recommended one, but a similar configuration process could be followed for the rest of the mentioned software.</p> <ol> <li> <p>Open the MobaXterm software</p> </li> <li> <p>Go to Sessions tab and select New session from the drop-down menu.</p> </li> <li> <p>Select the first icon (SSH) from the line-list on top of the pop-up window, and fill the Remote Host gap with the login server name (i.e. agamede.lgp.ehu.es )</p> </li> <li> <p>Tick the Specify username box and insert the Arina username of the user in the gap next to it, and press OK button.</p> </li> <li> <p>It will appear a new terminal windows asking for the required password.  Once you enter the corret password, the connection will be established and you will be able to use command line options as well as use \"Drag &amp; Drop\" to copy files from your personal PC to the server and vice-versa.</p> </li> </ol>"},{"location":"contact/about/","title":"ARINA Team","text":""},{"location":"contact/about/#management-and-support","title":"Management and Support","text":"<p>Our operations are driven by a team of dedicated professionals who offer a variety of services to ensure a seamless high-performance computing environment. If you have any questions, concerns, or suggestions, we encourage you to contact us through our general support email at izo-sgi@ehu.eus. For inquiries directed to a specific team member, their contact details are provided below.</p>"},{"location":"contact/about/#dr-joaquim-jornet-somoza","title":"Dr. Joaquim JORNET SOMOZA","text":"<p>  Dr. Joaquim Jornet Somoza, ORCID: 0000-0002-6721-1393, holds a PhD in Theoretical and Computational Chemistry from the University of Barcelona (2010). His scientific work has spanned various fields including Molecular Magnetism, Quantum Dynamics, Excitons, Neural Network models, and more recently, Quantum Computing\u2014all closely connected to the use of HPC technologies and infrastructures. He has published over 20 articles in Q1 journals, with more than 1200 citations and an h-index of 16. Since 2019, he has been enroled as technician at the Scientific Computing Service of the University of the Basque Country (UPV/EHU), and he was recently appointed as a BasQ Advocate to promote and support quantum computing within the Basque research ecosystem.</p> <p>email:      izo-sgi@ehu.eus</p> <p>phone:      943 01 8554 </p> <p>location:   Joxe Mari Korta bld,             Av.Tolosa 72, 2nd Floor;              Donostia             Gipuzkoa Campus</p>"},{"location":"contact/about/#dr-luca-bergamini","title":"Dr. Luca BERGAMINI","text":"<p> Dr. Luca Bergamini, ORCID: 0000-0001-7786-1499, earned his PhD in Physics in 2014 and has over 15 years of experience using HPC resources to support his theoretical research in nano-optics and nanotechnology. His career has developed in an international context, with work conducted in Italy, Spain, and the United States. In addition to his research, in 2022 he joined  the Scientific Computing Service as HPC infrastructure administrator and support team at SGIker . His scientific record includes a publication in Light: Science and Applications (IF 14.098, 69 citations), part of the Nature group, and another in Nano Letters (IF 12.712, 38 citations), along with 11 other papers\u2014all published in Q1 journals. Notably, he has delivered four invited talks at four of the 21 high-level national and international conferences he has attended.</p> <p>email:      izo-sgi@ehu.eus</p> <p>phone:      946 01 3542 </p> <p>location:   Ed. Vicerrectorado              Barrio Sarriena s/n             Leioa             Bizkaia Campus </p>"},{"location":"contact/contact/","title":"Contact Information","text":""},{"location":"contact/contact/#management-and-support","title":"Management and Support","text":"<p>The main channel to conctact with the ARINA support team is sending an email to : </p> <p>izo-sgi@ehu.eus</p> <p>This email is shared amount the all of us to provide a better user experience, since we will be able to solve any issue as soon as possible. </p> <p>Addionally, you can reach our speciallist :</p> <p>Dr. Joaquim JORNET SOMOZA</p> <p>phone:      943 01 8554 </p> <p>location:    Joxe Mari Korta building, Av.Tolosa 72, 2nd Floor;        Donostia;          Gipuzkoa Campus</p> <p>Dr. Luca BERGAMINI</p> <p>phone:      946 01 3542 </p> <p>location:   Ed. Vicerrectorado;          Barrio Sarriena s/n;         Leioa;         Bizkaia Campus </p>"},{"location":"hardware/filesystems/","title":"File Systems","text":"<p> Information</p> <p>This page is under construction.</p>"},{"location":"hardware/filesystems/#parallel-file-systems-beegfs","title":"Parallel File Systems : BeeGFS","text":"Connectivity of the BeeGFS files systems in the ARINA cluster"},{"location":"hardware/filesystems/#parallel-file-systems-lustre","title":"Parallel File Systems : LUSTRE","text":"LUSTRE files systems in the ARINA-ZAHAR cluster"},{"location":"hardware/network/","title":"Network / Connectivity","text":"<p> Information</p> <p>This page is under construction.</p>"},{"location":"hardware/nodes/","title":"Compute Nodes and Specialized Nodes","text":"<p>In this section, you will find detailed information about the compute nodes and specialized nodes available in each of our HPC clusters. This includes the total number of nodes, processor types, memory configurations, and any specialized hardware such as GPU and high-memory nodes.</p>"},{"location":"hardware/nodes/#cluster-1-arina","title":"Cluster 1: ARINA","text":"<p>This is the newest cluster in the IZO-SGI Scientific Computing Service. The new ARINA cluster was created in 2020 and upgraded in 2024.</p> <p>The following tabs shows the different types and features of the computing nodes:</p> Login nodesCompute NodesSpecialized Nodes: FAT &amp; GPUs <p>The cluster can be accessed connecting via ssh to the following login nodes :</p> <pre><code>1. agamede.lgp.ehu.es\n2. kalk2020.lgp.ehu.es\n</code></pre> <p>The ARINA cluster is currently composed by two set of compute node, named:   amd192 and xeon40</p> <p>adm192</p> <ul> <li>Total Number of Nodes: 22 ( named : naXX )</li> <li>Processor Type: 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>Number of Cores per Node: 192 </li> <li>Total Number of Cores: 3456</li> <li>Memory per Node: 4Gb / core = 768 Gb</li> <li>Total Memory: 13,5 Tb</li> </ul> <p>xeon40</p> <ul> <li>Total Number of Nodes: 44 (named nhXXX )</li> <li>Processor Type:  2 x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz with 20 cores</li> <li>Number of Cores per Node: 40 </li> <li>Total Number of Cores: 1760</li> <li>Memory per Node: 4,75Gb / core = 190 Gb</li> <li>Total Memory: 8,1 Tb</li> </ul> <p>A100 GPU Node</p> <ul> <li>Number of GPU Nodes: 1  ( named nagpu01 )</li> <li>Processor type : 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>GPU Model: NVIDIA A100 80Gb with NVlink pair connection. </li> <li>Number of GPUs per Node: 8</li> <li>MIG Partitioning: 4 x A100, 4 x 4g.40gb, 4 x 2g.20gb, 4 x 1g.20gb </li> <li>Memory per node: 1.5Tb </li> </ul> <p>H100 GPU Node</p> <ul> <li>Number of GPU Nodes: 1  ( named nagpu02 )</li> <li>Processor type : 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>GPU Model: NVIDIA H100 96Gb with NVlink pair connection. </li> <li>Number of GPUs per Node: 4</li> <li>MIG Partitioning: 2 x H100, 2 x 4g.47gb, 2 x 2g.24gb, 2 x 1g.24gb </li> <li>Memory per node: 1.5Tb </li> </ul> <p>High-Memory Nodes (FAT-node)</p> <ul> <li>Number of High-Memory Nodes: 1 ( name nh045, however GPU node can be used also as a FAT-node)</li> <li>Processor Type:  2 x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz with 20 cores</li> <li>Memory per High-Memory Node: 1,5Tb.</li> </ul>"},{"location":"hardware/nodes/#cluster-2-arina-zahar","title":"Cluster 2: ARINA-ZAHAR","text":"<p>The ARINA-ZAHAR cluster was created in 2015 and upgraded in 2017.</p> <p>The following tabs shows the different types and features of the computing nodes:</p> Login nodesCompute NodesSpecialized Nodes <p>The cluster can be accessed connecting via ssh to the following login nodes : </p> <pre><code>1. kalk2017.lgp.ehu.es\n2. katramila.lgp.ehu.es\n</code></pre> <p>The ARINA cluster is currently composed by two set of compute node, named:   amd192 and xeon40</p> <p>xeon28</p> <ul> <li>Total Number of Nodes: 67 ( named : ndXX )</li> <li>Processor Type: 2 x Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz Processor with 14 cores</li> <li>Number of Cores per Node: 28 </li> <li>Total Number of Cores: 1876</li> <li>Memory per Node: 4.4Gb / core = 125 Gb</li> <li>Total Memory:  8.2Tb</li> </ul> <p>xeon20</p> <ul> <li>Total Number of Nodes: 35 (named nbXXX )</li> <li>Processor Type:  2 x Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHzwith 10 cores</li> <li>Number of Cores per Node: 20 </li> <li>Total Number of Cores: 700</li> <li>Memory per Node: 6.4Gb / core = 128 Gb</li> <li>Total Memory: 4.4 Tb</li> </ul> <p>GPU Nodes</p> <ul> <li>Number of GPU Nodes: 1  ( named nd15 )</li> <li>Processor type : 2 x Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz processor with 14 cores</li> <li>GPU Model: NVIDIA Tesla K40m 11Gb. </li> <li>Number of GPUs per Node: 2 </li> <li>Memory per node: 110G </li> </ul> <p>High-Memory Nodes (FAT-node)</p> <ul> <li>Number of High-Memory Nodes: 2 ( name nb31, however GPU node can be used also as a FAT-node)</li> <li>Processor Type:  2 x  Intel(R) Xeon(R) CPU E5-4620 v2 @ 2.60GHzwith 16 cores</li> <li>Memory per High-Memory Node: 1,5Tb.</li> </ul>"},{"location":"hardware/resources/","title":"Overview","text":"<p>The High-Performance Computing (HPC) cluster , aka ARINA, at the University of the Basque Country (UPV/EHU) is a state-of-the-art computational resource designed to support a wide range of scientific and engineering research activities. This cluster provides researchers with the computational power necessary to perform complex simulations, data analysis, and other resource-intensive tasks.</p> <p>Our HPC cluster is equipped with the latest hardware and software technologies, ensuring high performance, reliability, and scalability. It is an essential tool for advancing research in fields such as physics, chemistry, biology, engineering, and more.</p> <p>Key features of our HPC cluster include:</p> <ul> <li>A large number of compute nodes with high-performance processors.</li> <li>Advanced network connectivity for fast data transfer and communication.</li> <li>High-capacity parallel file systems for efficient data storage and retrieval.</li> <li>A comprehensive software environment with a wide range of scientific applications and libraries.</li> </ul> <p>The HPC cluster is managed by a dedicated team of experts who provide user support, training, and documentation to help researchers make the most of this powerful resource. Our goal is to facilitate cutting-edge research and innovation by providing access to world-class computational infrastructure.</p> <p>You can find a detailed description of the nodes characteristics, network connectivy and file systems in the following links: </p> <ul> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"hardware/resources/#compute-nodes-and-specialized-nodes","title":"Compute nodes and Specialized nodes","text":""},{"location":"hardware/resources/#network-connectivity","title":"Network connectivity","text":""},{"location":"hardware/resources/#file-systems-home-directory-and-parallel-file-systems","title":"File systems : home directory and parallel file systems","text":""},{"location":"jobs/batch_scripts/","title":"BATCH Scripts in ARINA","text":"<p>Our aim is to help researchers:</p> <p>We prepared several BATCH scripts available in:</p> <p><code>/home/users/slurm/batch-scripts/</code></p> SOFTWARE ABINIT BLAST generic Lumerical nf_core_eager qespresso scilab starccm+ ams CP2K gromacs matlab nwchem qiime2 shasta Trinity ANSYS lammps molcas orca R siesta vasp"},{"location":"jobs/batch_scripts/#example-slurm-script-slurmsl","title":"Example SLURM Script (<code>slurm.sl</code>)","text":"<p>The following example shows a general BATCH script that can be modified or adapted to most of the common calculations ran in ARINA:</p> <pre><code>#!/bin/bash \n#SBATCH --ntasks=4\n#SBATCH --mem-per-cpu=3760M\n#SBATCH --time=4:00:00\n#SBATCH --partition=vfast\n\n# Required input files for the software execution,\n# separated by a space\nfiles=\"input.inp checkpoint.chk\"\n\n# Unload all possible present modules\nmodule purge\n# LOAD NOW REQUIRED MODULES\nmodule load MYDESIREDMODULE/version-compiler\n\n# Define the command line you will run\ncommand=\"\"\n\n#######################################################\n## Do not change below this line\n\n. /home/users/slurm/etc/slurm_func.sh\n\n# Signal trap to recover files\ntrap 'cleanup_function' EXIT\n\nHOST=$(hostname)\necho \"Job running on $HOST\"\n\nchooseScr\n\nLDIR=$(pwd)\nexport LDIR\n\necho \"cp ${files} $0 to $scr/.\"\ncp ${files} $0 ${scr}/.\n\ncd $scr\n\n# CALL EXECUTABLE with SRUN\necho /usr/bin/time -p srun ${command}\neval /usr/bin/time -p srun ${command}\n</code></pre> <p>Once the script is generated or modified, the <code>sbatch</code> command has to be executed in order to send the job to the cluster queues.</p> <p>Submitting a job</p> <pre><code>[user@agamede] sbatch slurm.sl\n</code></pre>"},{"location":"jobs/batch_scripts/#notes","title":"Notes","text":"<ul> <li>The scripts are designed for ARINA's SLURM-managed clusters.</li> <li><code>slurm_func.sh</code> contains helper functions for scratch space and file recovery.</li> <li><code>chooseScr</code> accepts the <code>-g</code> option to enforce to use the <code>/gscratch/$USER</code> directory.</li> <li>Always edit <code>command</code> and <code>files</code> variables according to the specific software you are running.</li> <li>Modules should be loaded as needed before execution.</li> </ul>"},{"location":"jobs/interactive/","title":"Interactive sessions","text":"<p> Information</p> <p>This page is under construction.</p> <p>It is not allowed to run any type of calculation into the login nodes, in order to avoid  risks that may effect other users or the global behaviour of the cluster. </p> <p>For that reason, for users that must work interactively with a software, we have enabled an option to  book the required resource to run in an interactive mode into a compute node. </p>"},{"location":"jobs/interactive/#the-interactive-command","title":"The <code>interactive</code> command","text":"<p>The <code>interactive</code>command enable the user to ask for a desired amount of resources, such a number of processors, memory, TRES, etc.</p> <p>The user can check all the options by executing</p> <pre><code>[user@agamede:~]$ interactive --help\n/home/users/slurm/bin/interactive: illegal option -- -\nUsage: interactive [-p] [-a] [-N] [-n] [-c] [-m] [-r] [-J] [-e] [-t] [-X] [-f] [-C] \n\nOptional arguments:\n -p: partition (default: vfast, values vfast p_fast p_medium p_slow p_sslow)\n -N: number of nodes (default: none)\n -n: number of tasks (default: 1)\n -c: number of CPU cores per task (default: 1)\n -f: node family (default: amd192, values in arina:amd192,xeon40 , in arina-zahar: xeon28, xeon20)\n -m: amount of memory (GB) per core (default: 1 [GB])\n -e: email address to which the begin session notification is to be sent\n -r: specify a reservation name\n -w: target node\n -g: add gres. E.g: gpu:1\n -J: job name\n -t: requested time in the format HH:MM:SS . Notice: it should be in accordance to queue limits \n -X: enable x11 display export.\n -C: enable to execute a command once the allocation is done.\n -b: locate the best node (only for default -N 1 )  \n     NOTICE: the -b option must be placed at the end\nDEFAULT : interactive -n 1 -c 1 -m 1 -J interactive -p vfast -t 4:00:00 \nexample : interactive -p 20_cores -c 4 -J MyFirstInteractiveJob\nexample : interactive -c 4 -J MyFirstInteractiveJob\n\nWritten by: Alan Orth &lt;a.orth@cgiar.org&gt;\nModified by: Jordi Blasco &lt;jordi.blasco@hpcnow.com&gt;;\n      and Joaquim Jornet &lt; joaquim.jornet@ehu.es&gt;;\n</code></pre> <p>Once the resources are granted, the user can execute the software like from the command line. </p> <p>Remember</p> <p>You must first load the desired module in order to set the environment variables needed for your calculation.</p>"},{"location":"jobs/interactive/#examples","title":"Examples","text":"<p>By default, if you execute the <code>interactive</code> command without any option, it will request to allocate just 1 CPU and 1Gb of memory.  This amount of memory usually is not enough to run your tests and you must select a larger amount. </p> <p>Here below, we show some examples depending on the needs of your calculation. </p> <p>Important</p> <p>It is important to do not run from the <code>$HOME</code> (<code>/home/$USER</code>) directory. Instead, create a temporally directory to <code>/scratch/$USER/$SLURM_JOB_ID</code>, or <code>/gscratch/$USER/$SLURM_JOB_ID</code>, copy your data  and move to that directory, and execute from there. Finally transfer your data back.</p> CPUs &amp; MemGPUX11: Export display <p>In order to request for N cpus and M Gb of memory per core:</p> <p><pre><code>[user@agamede:~]$ interactive  -n 4 -m 3 -J myjob \nRunning salloc command: \n/usr/bin/salloc  -n 4 -c 1 --mem-per-cpu=3072 -J myjob -p vfast -t 4:00:00     \nsalloc: Pending job allocation 345665\nsalloc: job 345665 queued and waiting for resources\nsalloc: job 345665 has been allocated resources\nsalloc: Granted job allocation 345665\nsalloc: Waiting for resource configuration\nsalloc: Nodes na02 are ready for job\n[user@na02:~]$ \n</code></pre> This comamand allocate the first 4 CPUs available with 3Gb of RAM per cpus, in the vfast partition and for a default time of 4h.</p> <p>You can see that the prompt of the host changes, from agamede to the name of a compute node, for instance : na02 . </p> <p>Now the user can load any available module, and execute the software from the command line. </p> <pre><code>[user@na02:~]$ mkdir /scratch/$USER/$SLURM_JOB_ID\n[user@na02:~]$ cp helloworld.c to /scratch/$USER/$SLURM_JOB_ID\n...\n[user@na02:/scratch/$USER$/345665]$ module load foss/2025a\n[user@na02:/scratch/$USER/345665]$ mpicc -o helloworld.exe  helloworld.c\n...\n[user@na02:/scratch/$USER/345665]$ cp helloworld /home/$USER/bin/\n</code></pre> <p>To request for N cpus , M Gb of memory per CPU and 1 NVIDIA A100 GPU card:</p> <p><pre><code>[user@agamede:~]$ interactive  -n 4 -m 3 -J mygpujob -g gpu:A100:1 \nRunning salloc command: \n/usr/bin/salloc  -n 4 -c 1 --mem-per-cpu=3072 -J mygpujob -p vfast -t 4:00:00 --gres=gpu:A100:1     \nsalloc: Pending job allocation 345737\nsalloc: job 345737 queued and waiting for resources\nsalloc: job 345737 has been allocated resources\nsalloc: Granted job allocation 345737\nsalloc: Waiting for resource configuration\nsalloc: Nodes nagpu01 are ready for job\n[user@nagpu01:~]$ \n</code></pre> This comamand will allocate 4 CPUs and 12Gb of memory RAM in a node (3Gb x 4 CPUs), and 1 NVIDIA A100 card in the vfast partition and for a default time of 4h.</p> <p>You can see that the prompt of the host changes, from agamede to the name of a compute node, for instance : nagpu01 . </p> <p>Now the user can load any available module, and execute the software from the command line. </p> <pre><code>[user@nagpu01:~]$ mkdir -p /scratch/$USER/$SLURM_JOB_ID\n[user@nagpu01:~]$ cp helloworld.cuda /scratch/$USER/$SLURM_JOB_ID\n[user@nagpu01:~]$ cd /scratch/$USER/$SLURM_JOB_ID\n[user@nagpu01:~]$ module load CUDA/12.8.0 \n[user@nagpu01:~]$ which nvcc \n/eb/x86_64/software/CUDA/12.8.0/bin/nvcc\n[user@nagpu01:~]$ nvcc -g -gencode arch=compute_80,code=sm_80 -lcudart -lcufft -lcublas -o helloworld helloworld.cuda\n[user@nagpu:/scratch/$USER/345665]$ cp helloworld /home/$USER/bin/\n</code></pre> <p>In order to request exporting the X11 display, one request for 1 CPU and M Gb of memory per core:</p> <p><pre><code>[user@agamede:~]$ interactive -X -n 1 -m 3 -J myX11job   \nRunning salloc command: \n/usr/bin/salloc --x11 -n 1 -c 1 --mem-per-cpu=3072 -J myX11job -p vfast -t 4:00:00     \nsalloc: Pending job allocation 345828\nsalloc: job 345828 queued and waiting for resources\nsalloc: job 345828 has been allocated resources\nsalloc: Granted job allocation 345828\nsalloc: Waiting for resource configuration\nsalloc: Nodes na01 are ready for job\n[user@na01:~]$ \n</code></pre> This comamand allocate the first 1 CPUs available with 3Gb of RAM per cpus, in the vfast partition and for a default time of 4h, allowing to export DISPLAY windows of the executed software</p> <p>Now the user can load any available module, and execute the software from the command line. </p> <pre><code>[user@na01:~]$ module load Molden \n[user@na01:~]$ molden\n</code></pre> <p>And the window opens like:   </p> <p>In order to finish an interactive session, the user just have to execute the <code>exit</code> command. </p> <pre><code>[user@na02:/scratch/$USER/345665]$ exit\nexit\nsrun: error: na02: task 0: Exited with exit code 130\nsrun: Terminating StepId=345665.interactive\nsalloc: Relinquishing job allocation 345665\nsalloc: Job allocation 345665 has been revoked.\n[user@agamede:~]$ \n</code></pre>"},{"location":"jobs/modules/","title":"Software manager: Modules (LMOD)","text":"<p>Modules</p> <p>This page will give you some insigths on how to use the LMOD software manager.</p> <p>The ARINA HPC system uses the LMOD (Lua-based Modules) system to manage user environments. This allows users to dynamically load or unload software packages in a clean and modular way.</p> <p>Using LMOD, users can:</p> <ul> <li>Switch between software versions easily.</li> <li>Load only the software they need.</li> <li>Avoid conflicts between packages and dependencies.</li> <li>Set up reproducible environments for jobs.</li> </ul>"},{"location":"jobs/modules/#what-is-a-module","title":"What is a Module?","text":"<p>A module is a small script that sets environment variables (like <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, etc.) so that software can be used on the command line.</p> <p>Modules are loaded or unloaded dynamically using commands like <code>module load</code>, <code>module unload</code>, or <code>ml</code> (a shorthand for module).</p>"},{"location":"jobs/modules/#common-module-commands","title":"Common Module Commands","text":"<p>Below are the most commonly used commands in the LMOD system.</p>"},{"location":"jobs/modules/#list-available-modules","title":"List Available Modules","text":"<p>You can list all modules currently available in the system using the following command:</p> <p><pre><code>[user@agamede:~]$ module avail\n</code></pre> or using the shorthand: <pre><code>[user@agamede:~]$ ml av\n</code></pre></p> <p>You can filter by name:</p> <pre><code>[user@agamede:~]$ ml av GROMACS\n\n----------------------------------------------------------------------------------- /eb/x86_64/modules/bio -----------------------------------------------------------------------------------\n   GROMACS/2023.3-aocc-4.1.0-openmpi-4.1.6-spack    GROMACS/2024.1-foss-2023b-CUDA-12.4.0    GROMACS/2024.1-foss-2023b (D)\n\n  Where:\n   D:  Default Module\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre>"},{"location":"jobs/modules/#show-information-about-a-module","title":"Show information about a Module","text":"<pre><code>[user@agamede:~]$ module show GROMACS/2024.1-foss-2023b\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   /eb/x86_64/modules/bio/GROMACS/2024.1-foss-2023b.lua:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the\nNewtonian equations of motion for systems with hundreds to millions of\nparticles.\n\nThis is a CPU only build, containing both MPI and threadMPI binaries\nfor both single and double precision.\n\nIt also contains the gmxapi extension for the single precision MPI build.\n</code></pre>"},{"location":"jobs/modules/#search-for-modules","title":"Search for Modules","text":"<p>The list of the installed software in the cluster is exhaustive. If you want to find if a specific software is installed, you can use  <code>module spider *software*</code> command. For example: </p> <p><pre><code>[user@agamede:~]$ module spider Python\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Python:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n     Versions:\n        Python/3.11.3-GCCcore-12.3.0\n        Python/3.11.5-GCCcore-13.2.0\n        Python/3.11.6-aocc-4.1.0-spack\n     Other possible modules matches:\n        GitPython  Python-bundle-PyPI  flatbuffers-python  meson-python  protobuf-python  python  python-isal  spglib-python  wxPython\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> Finds all versions of <code>Python</code> available, even those hidden in the module hierarchy.</p> <p>Usage limitation</p> <p>The login nodes are not intended for running any type of calculations. For that reason, it is ONLY allowed to search and show details of the available software.  In order to used, either you must load the modules in a batch script or start an interactive session in a compute node. </p>"},{"location":"jobs/modules/#load-a-module","title":"Load a Module","text":"<p>The following command enable you to load the specified software module into your environment. For example, if you are interested in SciPy python bundle :</p> <p><pre><code>module load SciPy-bundle\n</code></pre> or  <pre><code>ml  SciPy-bundle\n</code></pre></p> <p>This command will modify the necessary environment variables (PATH, LD_LIBRARY_PATH, etc) to be able to locate the executable and library to run the software. </p>"},{"location":"jobs/modules/#list-loaded-modules","title":"List Loaded Modules","text":"<p>In order to list all loaded modules, either directely loaded or dependencies, one can use <code>module list</code> command or the abbreviated command `ml' . </p> <pre><code>$ module load SciPy-bundle\n$ module list   # or ml\n\nCurrently Loaded Modules:\n  1) GCCcore/13.2.0                 6) FlexiBLAS/3.3.1-GCC-13.2.0  11) libreadline/8.2-GCCcore-13.2.0  16) OpenSSL/1.1                         21) Python-bundle-PyPI/2023.10-GCCcore-13.2.0\n  2) zlib/1.2.13-GCCcore-13.2.0     7) FFTW/3.3.10-GCC-13.2.0      12) Tcl/8.6.13-GCCcore-13.2.0       17) Python/3.11.5-GCCcore-13.2.0        22) pybind11/2.11.1-GCCcore-13.2.0\n  3) binutils/2.40-GCCcore-13.2.0   8) gfbf/2023b                  13) SQLite/3.43.1-GCCcore-13.2.0    18) cffi/1.15.1-GCCcore-13.2.0          23) SciPy-bundle/2023.11-gfbf-2023b\n  4) GCC/13.2.0                     9) bzip2/1.0.8-GCCcore-13.2.0  14) XZ/5.4.4-GCCcore-13.2.0         19) cryptography/41.0.5-GCCcore-13.2.0\n  5) OpenBLAS/0.3.24-GCC-13.2.0    10) ncurses/6.4-GCCcore-13.2.0  15) libffi/3.4.4-GCCcore-13.2.0     20) virtualenv/20.24.6-GCCcore-13.2.0\n</code></pre>"},{"location":"jobs/modules/#unload-all-modules","title":"Unload All Modules","text":"<p>In order to unload any module, it is recommended to purge all previsously loaded modules with the command <code>module purge</code>.</p> <p><pre><code>$ module purge\n</code></pre> This command will reset all the environment variables modified by <code>module load</code> command. </p>"},{"location":"jobs/modules/#understanding-module-hierarchy","title":"Understanding Module Hierarchy","text":"<p>The modules are strcutured as follows :   <code>SOFTWARE/version-compiler_toolchain-version-extra</code></p> <p>For example for : <code>SciPy-bundle/2023.11-gfbf-2023b</code> </p> <ul> <li> <p>Name of the software:   SciPy-bundle</p> </li> <li> <p>Version:                2023.11</p> </li> <li> <p>Compiler tool chain:    gfbf     </p> </li> <li> <p>Tool chain version: 2023b</p> </li> </ul> <p>If no version of the software is specified, the module manager will load the latest installed version. </p> <pre><code>[user@agamede:~]$ ml av ORCA     \n\n---------------------------------------- /eb/x86_64/modules/chem ------------------------------------------\n   ORCA/5.0.4-gompi-2023a    ORCA/5.0.4-gompi-2023b    ORCA/6.0.0-gompi-2023b    ORCA/6.0.1-gompi-2023b (D)\n\n  Where:\n   D:  Default Module\n</code></pre> <p>In that case if no version of <code>ORCA</code> software is not specified, the <code>module load ORCA</code> will load the 6.0.1 version specify by <code>(D)</code>.</p>"},{"location":"jobs/modules/#best-practice","title":"Best Practice","text":"<p>In the following we stress some important remarks to be token into account for a proper usage of the module software manager: </p> <ul> <li> <p>Use <code>ml purge</code> before starting a new setup to avoid environment conflicts.</p> </li> <li> <p>Always specify the full version of a module (e.g., <code>ml GCC/13.2.0</code>) for reproducibility.</p> </li> <li> <p>Use <code>module spider</code> if a module is not visible \u2014 it may be hidden due to the hierarchy.</p> </li> <li> <p>Include <code>module load</code> commands in job scripts for consistency and portability.</p> </li> </ul>"},{"location":"jobs/slurm/","title":"SLURM: job manager","text":"<p>SLURM</p> <p>This guide provides instructions on how to submit batch jobs using the SLURM scheduler on IZO-SGI's computing systems.</p>"},{"location":"jobs/slurm/#slurm-overview","title":"SLURM Overview","text":"<p>SLURM (Simple Linux Utility for Resource Management) is an open-source, fault-tolerant, and highly scalable system for managing clusters and scheduling batch jobs on both small and large Linux clusters.</p>"},{"location":"jobs/slurm/#batch-jobs","title":"Batch Jobs","text":"<p>Batch jobs are usually submitted via a batch script \u2014 a plain text file containing a set of job directives along with GNU/Linux commands or utilities. These scripts are submitted to SLURM, where they are placed in a queue and executed when the requested resources become available.</p>"},{"location":"jobs/slurm/#slurm-directives","title":"SLURM Directives","text":"<p>SLURM provides a wide range of directives to specify resource requirements and other job attributes. These directives can be included in:</p> <ul> <li> <p>Batch script headers (lines beginning with <code>#SBATCH</code>)</p> </li> <li> <p>Command-line options when using the <code>sbatch</code> command</p> </li> </ul> <p>Directives allow you to control various aspects of job execution, such as CPU and memory allocation, job name, output files, and execution time.</p> <p>In the following we describe the minimal BATCH options and some usefull and common variables: </p>"},{"location":"jobs/slurm/#batch-examples","title":"BATCH examples","text":"<p>Minimal Examples</p> <p>In this section we show only minimal example to run in different parallelization protocols. </p> <p>For more specific batch scripts, please take a look on Batch scitps section.</p> SerialPure MPI / Distributed memoryPure OpenMPI / Thread ParalleizationHybrid MPI-OpenMPI  <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n\nmodule load MYPROGRAM\nmyprogram\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre>"}]}