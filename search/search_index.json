{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to ARINA:  The Scientific Computing Facility of the IZO-SGIker (UPV/EHU)","text":""},{"location":"#aim","title":"AIM","text":"<p>The Scientific Computing Service provides high performance computing resources, specialized technical support and consulting to researchers at the UPV/EHU, other public organizations and companies. The service provides its own computing resources with powerful hardware, upgrade policies and distributes the computational resources. The service analyzes new tendencies in HPC (High Performance Computing) for its possible implementation in the UPV / EHU. The IZO-SGI is part of the General Research Services (SGIker) of the UPV / EHU, and therefore complies with the protocols established by the SGIker. All results published using the IZO-SGI resources should acknowledgment its usage and send a copy of the publication. This publications will be used to evaluate the users when prioritizing the use of the Service. The phrase we propose for the acknowledgments is:</p> <p>Link to the acknowledge phrase</p> <p>Like for the rest of the SGIker Services, the Governing Council of the University will annually approve the table of rates to be applied to the services with specifications of the different modalities, according to the nature and institutional relationship of the users.</p> <p>The rates, at a minimum, must cover the costs of consumables used in the provision and basic maintenance of the equipment and facilities.</p> <p>You can consult the Public Price Catalog at the last page of this link.</p>"},{"location":"contact/","title":"ARINA Team","text":""},{"location":"contact/#management-and-support","title":"Management and Support","text":"<p>Our operations are driven by a team of dedicated professionals who offer a variety of services to ensure a seamless high-performance computing environment. If you have any questions, concerns, or suggestions, we encourage you to contact us through our general support email at izo-sgi@ehu.eus. For inquiries directed to a specific team member, their contact details are provided below.</p>"},{"location":"contact/#dr-joaquim-jornet-somoza","title":"Dr. Joaquim JORNET SOMOZA","text":"<p>Dr. Joaquim Jornet Somoza, ORCID: 0000-0002-6721-1393, holds a PhD in Theoretical and Computational Chemistry from the University of Barcelona (2010). His scientific work has spanned various fields including Molecular Magnetism, Quantum Dynamics, Excitons, Neural Network models, and more recently, Quantum Computing\u2014all closely connected to the use of HPC technologies and infrastructures. He has published over 20 articles in Q1 journals, with more than 1200 citations and an h-index of 16. Since 2019, he has been enroled as technician at the Scientific Computing Service of the University of the Basque Country (UPV/EHU), and he was recently appointed as a BasQ Advocate to promote and support quantum computing within the Basque research ecosystem.</p> <p>email:      izo-sgi@ehu.eus</p> <p>phone:      943 01 8554 </p> <p>location:   Joxe Mari Korta bld,             Av.Tolosa 72, 2nd Floor;              Donostia             Gipuzkoa Campus</p>"},{"location":"contact/#dr-luca-bergamini","title":"Dr. Luca BERGAMINI","text":"<p>Dr. Luca Bergamini, ORCID: 0000-0001-7786-1499, earned his PhD in Physics in 2014 and has over 15 years of experience using HPC resources to support his theoretical research in nano-optics and nanotechnology. His career has developed in an international context, with work conducted in Italy, Spain, and the United States. In addition to his research, in 2022 he joined  the Scientific Computing Service as HPC infrastructure administrator and support team at SGIker . His scientific record includes a publication in Light: Science and Applications (IF 14.098, 69 citations), part of the Nature group, and another in Nano Letters (IF 12.712, 38 citations), along with 11 other papers\u2014all published in Q1 journals. Notably, he has delivered four invited talks at four of the 21 high-level national and international conferences he has attended.</p> <p>email:      izo-sgi@ehu.eus</p> <p>phone:      946 01 3542 </p> <p>location:   Ed. Vicerrectorado              Barrio Sarriena s/n             Leioa             Bizkaia Campus </p>"},{"location":"Access/account/","title":"ARINA accounts","text":"<p>To access the high-performance computing (HPC) resources at Scientific Computing Service of the SGIker (UPV/EHU), you need a valid ARINA cluster account. If you don't have one yet, please apply for an account.</p> <p>Although we can provide access to the ARINA cluster for all members of the scientific community, one have to tak e different procedures depending if the researcher belong or not to the University of the Basque Country:</p>"},{"location":"Access/account/#upvehu-researchers","title":"UPV/EHU Researchers","text":"<p>The request of an account have to be done by completing the required form available at the following link.</p> <p>Login with LDAP account of the UPV/EHU</p> <p>In order to access to the form, you need a valid EHU/UPV credentials. If you done have, please, contact the support and administration teams at izo-sgi@ehu.eus .</p> Time of calculation requestExperienced or Permanent researchersPhD, Master or other students <p>If your group still don't have an account in our service, we strongly recommend to fulfill the form \"Time of calculation request\" with the data of the new principal investigator and the information concerning the research line of the group. Besides, in order to consider your resource demands for future extensions of the service, we request you for an estimation of the CPU/GPU time. </p> <p>The data that will be required is: </p> <pre><code>- Head of the group (ID/DNI number (without letters)):\n- Computing time request (days) :\n- Title:\n- Summary of the research line:\n- References:\n</code></pre> <p>Experienced researchers, that have been authorized by the IP of the group, can directly apply for an account by selecting the \"New account request\" button from the left banner.</p> <p>The required data will be automatically loaded from the corporative LDAP account, and then you just have to select the head of the research group from the list.</p> <p>For PhD, Master or other kind of student, the form must be fulfilled by the IP of the group to guarantee the agreement of the usage condiction of the ARINA cluster. In that case, please choose the  \"New account request with guarantee\" button located on the left banner. </p> <p>The data that will be required is: </p> <pre><code>- ID/DNI/NIE number (without letters):\n- Name:                 First Surname:      Second Surname:\n- Telephone:            email: \n- Department:           Title: (Researcher/Student/Guest/Others)\n- Head of research group: (select one in the list)\n</code></pre>"},{"location":"Access/account/#opis-external-researchers","title":"OPIs &amp; External Researchers","text":"<p>Researchers of external companies have to contact the administrators to create an account to get access to the EHU/UPV network. </p> <p>Please, send an email with the following data to izo-sgi@ehu.eus : </p> <pre><code>- DNI/NIE: \n- Name:         First Surname:          Second Surname:\n- email: \n- Company and department:\n</code></pre> <p>Once you receive the credentials as invited researcher, you will be required to modify your password by accessing to BILATU. </p> <p>The next step will be to follow the same procedure described for the UPV/EHU members.</p> <p> </p> <p>Please note that account approvals are not automatic and are subject to an internal review process.</p>"},{"location":"Access/connect/","title":"Connection","text":""},{"location":"Access/connect/#ssh-protocol","title":"SSH Protocol","text":"<p>Secure Shell (SSH) is a cryptographic network protocol that facilitates secure remote access to computer systems or servers over insecure networks. It establishes a secure channel for data communication between two devices, protecting transmitted data from unauthorized access, tampering, and eavesdropping.</p> <p>SSH is extensively used for remote administration, secure file transfers, and other network services. Key benefits of SSH include:</p> <ul> <li>Encrypted Communication : SSH encrypts all data exchanged between the client and server, ensuring confidentiality and integrity.</li> <li>Strong Authentication: SSH supports various authentication methods, such as passwords, public key authentication, and Kerberos-based authentication.</li> <li>Flexibility: SSH can be integrated with different network protocols and applications, making it highly adaptable to various use cases.</li> </ul>"},{"location":"Access/connect/#ssh-in-the-context-of-hpc","title":"SSH in the Context of HPC","text":"<p>In High Performance Computing (HPC) environments, researchers and engineers frequently need to access remote clusters and supercomputers for complex simulations, data processing, and analysis. Using SSH in HPC systems offers several advantages:</p> <ul> <li>Security: HPC systems often handle sensitive research data, making security a top priority. SSH ensures that data and system access remain secure.</li> <li>Scalability: SSH enables users to manage and interact with multiple nodes in an HPC cluster, simplifying the deployment and control of large-scale computing resources.</li> <li>Remote Access: Users can access HPC systems from anywhere with an internet connection, enhancing collaboration and productivity.</li> </ul>"},{"location":"Access/connect/#login-servers-connection","title":"Login servers connection","text":"<p> Internal Network </p> <p>Notice that before trying to connect via '''ssh''' to any of the ARINA's login servers, you must be connected into the EHU/UPV network. So, either first you connect from a PC located in the EHU/UPV or you must first connect to the VPN. For the latter, you can find information in the following link.</p> Linux / MacWindows <p>If the workstation employed for logging in Arina deploys either a Unix/linux or a Mac OS operating system, then the sole ssh protocol can be used.</p> <p>Within the most common releases of these OSs, the ssh protocol is usually already activated.  If not, look for instructions on how to activate it on the web.</p> <p>You an connect easyl following the next steps:</p> <ol> <li> <p>Open a command line windows (by either clicking on the command line icon or typing command line in the OS browser bar)</p> </li> <li> <p>Use ssh command to start secure connectio to the server:</p> <pre><code>ssh -X username@agamede.lgp.ehu.es\n</code></pre> </li> </ol> <p>The -X flag allows for the opening of Graphical User Interfaces (GUIs), namely program windows. In some cases, if you are working on a Mac OS workstation, you may need a third-party application to be able to launch GUIs: XQuartz. After its download and installation, make sure to launch it before trying to log in Arina.</p> <p>In order to connect from a Windows PC, you will need to download and install a third party software that enable ssh and or ftp connection to a Linux server.  For its simplicity we recommend to install MobaXterm</p> <p>Here we show how to set up MobaXterm, which is the recommended one, but a similar configuration process could be followed for the rest of the mentioned software.</p> <ol> <li> <p>Open the MobaXterm software</p> </li> <li> <p>Go to Sessions tab and select New session from the drop-down menu.</p> </li> <li> <p>Select the first icon (SSH) from the line-list on top of the pop-up window, and fill the Remote Host gap with the login server name (i.e. agamede.lgp.ehu.es )</p> </li> <li> <p>Tick the Specify username box and insert the Arina username of the user in the gap next to it, and press OK button.</p> </li> <li> <p>It will appear a new terminal windows asking for the required password.  Once you enter the corret password, the connection will be established and you will be able to use command line options as well as use \"Drag &amp; Drop\" to copy files from your personal PC to the server and vice-versa.</p> </li> </ol>"},{"location":"Hardware/filesystems/","title":"File Systems","text":"<p> Information</p> <p>This page is under construction.</p>"},{"location":"Hardware/network/","title":"Network / Connectivity","text":"<p> Information</p> <p>This page is under construction.</p>"},{"location":"Hardware/nodes/","title":"Compute Nodes and Specialized Nodes","text":"<p>In this section, you will find detailed information about the compute nodes and specialized nodes available in each of our HPC clusters. This includes the total number of nodes, processor types, memory configurations, and any specialized hardware such as GPU and high-memory nodes.</p>"},{"location":"Hardware/nodes/#cluster-1-arina","title":"Cluster 1: ARINA","text":"<p>This is the newest cluster in the IZO-SGI Scientific Computing Service. The new ARINA cluster was created in 2020 and upgraded in 2024.</p> <p>The following tabs shows the different types and features of the computing nodes:</p> Login nodesCompute NodesSpecialized Nodes: FAT &amp; GPUs <p>The cluster can be accessed connecting via ssh to the following login nodes :</p> <pre><code>1. agamede.lgp.ehu.es\n2. kalk2020.lgp.ehu.es\n</code></pre> <p>The ARINA cluster is currently composed by two set of compute node, named:   amd192 and xeon40</p> <p>adm192</p> <ul> <li>Total Number of Nodes: 22 ( named : naXX )</li> <li>Processor Type: 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>Number of Cores per Node: 192 </li> <li>Total Number of Cores: 3456</li> <li>Memory per Node: 4Gb / core = 768 Gb</li> <li>Total Memory: 13,5 Tb</li> </ul> <p>xeon40</p> <ul> <li>Total Number of Nodes: 44 (named nhXXX )</li> <li>Processor Type:  2 x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz with 20 cores</li> <li>Number of Cores per Node: 40 </li> <li>Total Number of Cores: 1760</li> <li>Memory per Node: 4,75Gb / core = 190 Gb</li> <li>Total Memory: 8,1 Tb</li> </ul> <p>A100 GPU Node</p> <ul> <li>Number of GPU Nodes: 1  ( named nagpu01 )</li> <li>Processor type : 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>GPU Model: NVIDIA A100 80Gb with NVlink pair connection. </li> <li>Number of GPUs per Node: 8</li> <li>MIG Partitioning: 4 x A100, 4 x 4g.40gb, 4 x 2g.20gb, 4 x 1g.20gb </li> <li>Memory per node: 1.5Tb </li> </ul> <p>H100 GPU Node</p> <ul> <li>Number of GPU Nodes: 1  ( named nagpu02 )</li> <li>Processor type : 2 x AMD EPYC 9654 Processor with 96 cores</li> <li>GPU Model: NVIDIA H100 96Gb with NVlink pair connection. </li> <li>Number of GPUs per Node: 4</li> <li>MIG Partitioning: 2 x H100, 2 x 4g.47gb, 2 x 2g.24gb, 2 x 1g.24gb </li> <li>Memory per node: 1.5Tb </li> </ul> <p>High-Memory Nodes (FAT-node)</p> <ul> <li>Number of High-Memory Nodes: 1 ( name nh045, however GPU node can be used also as a FAT-node)</li> <li>Processor Type:  2 x Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz with 20 cores</li> <li>Memory per High-Memory Node: 1,5Tb.</li> </ul>"},{"location":"Hardware/nodes/#cluster-2-arina-zahar","title":"Cluster 2: ARINA-ZAHAR","text":"<p>The ARINA-ZAHAR cluster was created in 2015 and upgraded in 2017.</p> <p>The following tabs shows the different types and features of the computing nodes:</p> Login nodesCompute NodesSpecialized Nodes <p>The cluster can be accessed connecting via ssh to the following login nodes : </p> <pre><code>1. kalk2017.lgp.ehu.es\n2. katramila.lgp.ehu.es\n</code></pre> <p>The ARINA cluster is currently composed by two set of compute node, named:   amd192 and xeon40</p> <p>xeon28</p> <ul> <li>Total Number of Nodes: 67 ( named : ndXX )</li> <li>Processor Type: 2 x Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz Processor with 14 cores</li> <li>Number of Cores per Node: 28 </li> <li>Total Number of Cores: 1876</li> <li>Memory per Node: 4.4Gb / core = 125 Gb</li> <li>Total Memory:  8.2Tb</li> </ul> <p>xeon20</p> <ul> <li>Total Number of Nodes: 35 (named nbXXX )</li> <li>Processor Type:  2 x Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHzwith 10 cores</li> <li>Number of Cores per Node: 20 </li> <li>Total Number of Cores: 700</li> <li>Memory per Node: 6.4Gb / core = 128 Gb</li> <li>Total Memory: 4.4 Tb</li> </ul> <p>GPU Nodes</p> <ul> <li>Number of GPU Nodes: 1  ( named nd15 )</li> <li>Processor type : 2 x Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz processor with 14 cores</li> <li>GPU Model: NVIDIA Tesla K40m 11Gb. </li> <li>Number of GPUs per Node: 2 </li> <li>Memory per node: 110G </li> </ul> <p>High-Memory Nodes (FAT-node)</p> <ul> <li>Number of High-Memory Nodes: 2 ( name nb31, however GPU node can be used also as a FAT-node)</li> <li>Processor Type:  2 x  Intel(R) Xeon(R) CPU E5-4620 v2 @ 2.60GHzwith 16 cores</li> <li>Memory per High-Memory Node: 1,5Tb.</li> </ul>"},{"location":"Hardware/resources/","title":"Overview","text":"<p>The High-Performance Computing (HPC) cluster , aka ARINA, at the University of the Basque Country (UPV/EHU) is a state-of-the-art computational resource designed to support a wide range of scientific and engineering research activities. This cluster provides researchers with the computational power necessary to perform complex simulations, data analysis, and other resource-intensive tasks.</p> <p>Our HPC cluster is equipped with the latest hardware and software technologies, ensuring high performance, reliability, and scalability. It is an essential tool for advancing research in fields such as physics, chemistry, biology, engineering, and more.</p> <p>Key features of our HPC cluster include:</p> <ul> <li>A large number of compute nodes with high-performance processors.</li> <li>Advanced network connectivity for fast data transfer and communication.</li> <li>High-capacity parallel file systems for efficient data storage and retrieval.</li> <li>A comprehensive software environment with a wide range of scientific applications and libraries.</li> </ul> <p>The HPC cluster is managed by a dedicated team of experts who provide user support, training, and documentation to help researchers make the most of this powerful resource. Our goal is to facilitate cutting-edge research and innovation by providing access to world-class computational infrastructure.</p> <p>You can find a detailed description of the nodes characteristics, network connectivy and file systems in the following links: </p> <ul> <li> </li> <li> </li> <li> </li> </ul>"},{"location":"Hardware/resources/#compute-nodes-and-specialized-nodes","title":"Compute nodes and Specialized nodes","text":""},{"location":"Hardware/resources/#network-connectivity","title":"Network connectivity","text":""},{"location":"Hardware/resources/#file-systems-home-directory-and-parallel-file-systems","title":"File systems : home directory and parallel file systems","text":""},{"location":"Jobs/modules/","title":"Software manager: Modules (LMOD)","text":"<p>Modules</p> <p>This page will give you some insigths on how to use the LMOD software manager.</p> <p>The ARINA HPC system uses the LMOD (Lua-based Modules) system to manage user environments. This allows users to dynamically load or unload software packages in a clean and modular way.</p> <p>Using LMOD, users can:</p> <ul> <li>Switch between software versions easily.</li> <li>Load only the software they need.</li> <li>Avoid conflicts between packages and dependencies.</li> <li>Set up reproducible environments for jobs.</li> </ul>"},{"location":"Jobs/modules/#what-is-a-module","title":"What is a Module?","text":"<p>A module is a small script that sets environment variables (like <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, etc.) so that software can be used on the command line.</p> <p>Modules are loaded or unloaded dynamically using commands like <code>module load</code>, <code>module unload</code>, or <code>ml</code> (a shorthand for module).</p>"},{"location":"Jobs/modules/#common-module-commands","title":"Common Module Commands","text":"<p>Below are the most commonly used commands in the LMOD system.</p>"},{"location":"Jobs/modules/#list-available-modules","title":"List Available Modules","text":"<p>You can list all modules currently available in the system using the following command:</p> <p><pre><code>[user@agamede:~]$ module avail\n</code></pre> or using the shorthand: <pre><code>[user@agamede:~]$ ml av\n</code></pre></p> <p>You can filter by name:</p> <pre><code>[user@agamede:~]$ ml av GROMACS\n\n----------------------------------------------------------------------------------- /eb/x86_64/modules/bio -----------------------------------------------------------------------------------\n   GROMACS/2023.3-aocc-4.1.0-openmpi-4.1.6-spack    GROMACS/2024.1-foss-2023b-CUDA-12.4.0    GROMACS/2024.1-foss-2023b (D)\n\n  Where:\n   D:  Default Module\n\nIf the avail list is too long consider trying:\n\n\"module --default avail\" or \"ml -d av\" to just list the default modules.\n\"module overview\" or \"ml ov\" to display the number of modules for each name.\n\nUse \"module spider\" to find all possible modules and extensions.\nUse \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\".\n</code></pre>"},{"location":"Jobs/modules/#show-information-about-a-module","title":"Show information about a Module","text":"<pre><code>[user@agamede:~]$ module show GROMACS/2024.1-foss-2023b\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   /eb/x86_64/modules/bio/GROMACS/2024.1-foss-2023b.lua:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nhelp([[\nDescription\n===========\nGROMACS is a versatile package to perform molecular dynamics, i.e. simulate the\nNewtonian equations of motion for systems with hundreds to millions of\nparticles.\n\nThis is a CPU only build, containing both MPI and threadMPI binaries\nfor both single and double precision.\n\nIt also contains the gmxapi extension for the single precision MPI build.\n</code></pre>"},{"location":"Jobs/modules/#search-for-modules","title":"Search for Modules","text":"<p>The list of the installed software in the cluster is exhaustive. If you want to find if a specific software is installed, you can use  <code>module spider *software*</code> command. For example: </p> <p><pre><code>[qjornet@agamede:~]$ module spider Python\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Python:\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n     Versions:\n        Python/3.11.3-GCCcore-12.3.0\n        Python/3.11.5-GCCcore-13.2.0\n        Python/3.11.6-aocc-4.1.0-spack\n     Other possible modules matches:\n        GitPython  Python-bundle-PyPI  flatbuffers-python  meson-python  protobuf-python  python  python-isal  spglib-python  wxPython\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> Finds all versions of <code>Python</code> available, even those hidden in the module hierarchy.</p> <p>Usage limitation</p> <p>The login nodes are not intended for running any type of calculations. For that reason, it is ONLY allowed to search and show details of the available software.  In order to used, either you must load the modules in a batch script or start an interactive session in a compute node. </p>"},{"location":"Jobs/modules/#load-a-module","title":"Load a Module","text":"<p>The following command enable you to load the specified software module into your environment. For example, if you are interested in SciPy python bundle :</p> <p><pre><code>module load SciPy-bundle\n</code></pre> or  <pre><code>ml  SciPy-bundle\n</code></pre></p> <p>This command will modify the necessary environment variables (PATH, LD_LIBRARY_PATH, etc) to be able to locate the executable and library to run the software. </p>"},{"location":"Jobs/modules/#list-loaded-modules","title":"List Loaded Modules","text":"<p>In order to list all loaded modules, either directely loaded or dependencies, one can use <code>module list</code> command or the abbreviated command `ml' . </p> <pre><code>$ module load SciPy-bundle\n$ module list   # or ml\n\nCurrently Loaded Modules:\n  1) GCCcore/13.2.0                 6) FlexiBLAS/3.3.1-GCC-13.2.0  11) libreadline/8.2-GCCcore-13.2.0  16) OpenSSL/1.1                         21) Python-bundle-PyPI/2023.10-GCCcore-13.2.0\n  2) zlib/1.2.13-GCCcore-13.2.0     7) FFTW/3.3.10-GCC-13.2.0      12) Tcl/8.6.13-GCCcore-13.2.0       17) Python/3.11.5-GCCcore-13.2.0        22) pybind11/2.11.1-GCCcore-13.2.0\n  3) binutils/2.40-GCCcore-13.2.0   8) gfbf/2023b                  13) SQLite/3.43.1-GCCcore-13.2.0    18) cffi/1.15.1-GCCcore-13.2.0          23) SciPy-bundle/2023.11-gfbf-2023b\n  4) GCC/13.2.0                     9) bzip2/1.0.8-GCCcore-13.2.0  14) XZ/5.4.4-GCCcore-13.2.0         19) cryptography/41.0.5-GCCcore-13.2.0\n  5) OpenBLAS/0.3.24-GCC-13.2.0    10) ncurses/6.4-GCCcore-13.2.0  15) libffi/3.4.4-GCCcore-13.2.0     20) virtualenv/20.24.6-GCCcore-13.2.0\n</code></pre>"},{"location":"Jobs/modules/#unload-all-modules","title":"Unload All Modules","text":"<p>In order to unload any module, it is recommended to purge all previsously loaded modules with the command <code>module purge</code>.</p> <p><pre><code>$ module purge\n</code></pre> This command will reset all the environment variables modified by <code>module load</code> command. </p>"},{"location":"Jobs/modules/#understanding-module-hierarchy","title":"Understanding Module Hierarchy","text":"<p>The modules are strcutured as follows :   <code>SOFTWARE/version-compiler_toolchain-version-extra</code></p> <p>For example for : <code>SciPy-bundle/2023.11-gfbf-2023b</code> </p> <ul> <li> <p>Name of the software:   SciPy-bundle</p> </li> <li> <p>Version:                2023.11</p> </li> <li> <p>Compiler tool chain:    gfbf     </p> </li> <li> <p>Tool chain version: 2023b</p> </li> </ul> <p>If no version of the software is specified, the module manager will load the latest installed version. </p> <pre><code>[qjornet@agamede1:~]$ ml av ORCA     \n\n---------------------------------------- /eb/x86_64/modules/chem ------------------------------------------\n   ORCA/5.0.4-gompi-2023a    ORCA/5.0.4-gompi-2023b    ORCA/6.0.0-gompi-2023b    ORCA/6.0.1-gompi-2023b (D)\n\n  Where:\n   D:  Default Module\n</code></pre> <p>In that case if no version of <code>ORCA</code> software is not specified, the <code>module load ORCA</code> will load the 6.0.1 version specify by <code>(D)</code>.</p>"},{"location":"Jobs/modules/#best-practice","title":"Best Practice","text":"<p>In the following we stress some important remarks to be token into account for a proper usage of the module software manager: </p> <ul> <li> <p>Use <code>ml purge</code> before starting a new setup to avoid environment conflicts.</p> </li> <li> <p>Always specify the full version of a module (e.g., <code>ml GCC/13.2.0</code>) for reproducibility.</p> </li> <li> <p>Use <code>module spider</code> if a module is not visible \u2014 it may be hidden due to the hierarchy.</p> </li> <li> <p>Include <code>module load</code> commands in job scripts for consistency and portability.</p> </li> </ul>"},{"location":"Jobs/slurm/","title":"SLURM: job manager","text":"<p>SLURM</p> <p>This guide provides instructions on how to submit batch jobs using the SLURM scheduler on IZO-SGI's computing systems.</p>"},{"location":"Jobs/slurm/#slurm-overview","title":"SLURM Overview","text":"<p>SLURM (Simple Linux Utility for Resource Management) is an open-source, fault-tolerant, and highly scalable system for managing clusters and scheduling batch jobs on both small and large Linux clusters.</p>"},{"location":"Jobs/slurm/#batch-jobs","title":"Batch Jobs","text":"<p>Batch jobs are usually submitted via a batch script \u2014 a plain text file containing a set of job directives along with GNU/Linux commands or utilities. These scripts are submitted to SLURM, where they are placed in a queue and executed when the requested resources become available.</p>"},{"location":"Jobs/slurm/#slurm-directives","title":"SLURM Directives","text":"<p>SLURM provides a wide range of directives to specify resource requirements and other job attributes. These directives can be included in:</p> <ul> <li> <p>Batch script headers (lines beginning with <code>#SBATCH</code>)</p> </li> <li> <p>Command-line options when using the <code>sbatch</code> command</p> </li> </ul> <p>Directives allow you to control various aspects of job execution, such as CPU and memory allocation, job name, output files, and execution time.</p> <p>In the following we describe the minimal BATCH options and some usefull and common variables: </p>"},{"location":"Jobs/slurm/#batch-examples","title":"BATCH examples","text":"SerialPure MPI / Distributed memoryPure OpenMPI / Thread ParalleizationHybrid MPI-OpenMPI  <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks=1\n\nmodule load MYPROGRAM\nmyprogram\n</code></pre> <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre> <pre><code>#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=8\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre> <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load MYPROGRAM\nsrun myprogram\n</code></pre>"}]}